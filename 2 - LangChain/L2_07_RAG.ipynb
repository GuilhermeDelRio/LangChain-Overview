{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c48cdb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders.wikipedia import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef4c78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.0,\n",
    "  base_url=\"https://openai.vocareum.com/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f28618",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WikipediaLoader(\n",
    "  \"Anthony_Hopkins\",\n",
    "  load_max_docs=1,\n",
    "  doc_content_chars_max=4000\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ec6180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a0e734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d410af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir Philip Anthony Hopkins (born 31 December 1937) is a Welsh actor. Considered one of Britain's most recognisable and prolific actors, he is known for his performances on the screen and stage. Hopkins has received numerous accolades, including two Academy Awards, four BAFTA Awards, two Primetime Emmy Awards, and a Laurence Olivier Award. He has also received the Cecil B. DeMille Award in 2005 and the BAFTA Fellowship for lifetime achievement in 2008. He was knighted by Queen Elizabeth II for his services to drama in 1993.\n",
      "After graduating from the Royal Welsh College of Music & Drama in 1957, Hopkins trained at RADA (the Royal Academy of Dramatic Art) in London. He was then spotted by Laurence Olivier, who invited him to join the Royal National Theatre in 1965. Productions at the National included King Lear (his favourite Shakespeare play), Coriolanus, Macbeth, and Antony and Cleopatra. In 1985, he received acclaim and a Laurence Olivier Award for his performance in the David Hare play Pravda. His last stage play was a West End production of M. Butterfly in 1989.\n",
      "Hopkins's early film roles include The Lion in Winter (1968), A Bridge Too Far (1977), and The Elephant Man (1980). He won two Academy Awards for Best Actor for playing Hannibal Lecter in The Silence of the Lambs (1991) and an octogenarian with dementia in The Father (2020), becoming the oldest Best Actor Oscar winner for the latter. His other Oscar-nominated films include The Remains of the Day (1993), Nixon (1995), Amistad (1997), and The Two Popes (2019). Other notable films include 84 Charing Cross Road (1987), Howards End (1992), Bram Stoker's Dracula (1992), Shadowlands (1993), Legends of the Fall (1994), The Mask of Zorro (1998), and the Marvel Cinematic Universe's Thor films (2011–2017).\n",
      "For his work on television, Hopkins received a British Academy Television Award for Best Actor for his performance in War and Peace (1972). He won two Primetime Emmy Awards for Outstanding Actor in a Drama Series for The Lindbergh Kidnapping Case (1976) and The Bunker (1981). Other notable projects include the BBC film The Dresser (2015), PBS's King Lear (2018), and the HBO series Westworld (2016–2018).\n",
      "\n",
      "\n",
      "== Early life and education ==\n",
      "Philip Anthony Hopkins was born in the Margam district of Port Talbot, Wales, on 31 December 1937, the son of Annie Muriel (née Yeates) and baker Richard Arthur Hopkins. One of his grandfathers was from Wiltshire, England. He stated his father's working-class values have always underscored his life, \"Whenever I get a feeling that I may be special or different, I think of my father and I remember his hands – his hardened, broken hands.\" His school days were unproductive; he would rather immerse himself in art, such as painting and drawing, or playing the piano than attend to his studies. In 1949, to instil discipline, his parents insisted he attend Jones' West Monmouth Boys' School in Pontypool. He remained there for five terms and was then educated at Cowbridge Grammar School in the Vale of Glamorgan. In an interview in 2002, he stated, \"I was a poor learner, which left me open to ridicule and gave me an inferiority complex. I grew up absolutely convinced I was stupid.\"\n",
      "\n",
      "Hopkins was inspired by fellow Welsh actor Richard Burton, whom he met at the age of 15. He later called Burton \"very gracious, very nice\" but elaborated, \"I don't know where everyone gets the idea we were good friends. I suppose it's because we are both Welsh and grew up near the same town. For the record, I didn't really know him at all.\" He enrolled at the Royal Welsh College of Music & Drama in Cardiff, from which he graduated in 1957. He next met Burton in 1975 as Burton prepared to take over Hopkins's role as the psychiatrist in Peter Shaffer's Equus, with Hopkins stating, \"He was a phenomenal actor. So was Peter O'Toole – they were wonderful, larger-than-life characters.\" He spent two years doing his national service between 1958 and 1960, which he served in the British \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960279ee",
   "metadata": {},
   "source": [
    "Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db218fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=300,\n",
    "  chunk_overlap=50\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12092fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Wikipedia page into 17 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split Wikipedia page into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba44a4c",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d381eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", base_url=\"https://openai.vocareum.com/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447b3ca",
   "metadata": {},
   "source": [
    "Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af40fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe5d794d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m document_ids = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m document_ids[:\u001b[32m10\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_core/vectorstores/in_memory.py:195\u001b[39m, in \u001b[36mInMemoryVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, ids, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add documents to the store.\"\"\"\u001b[39;00m\n\u001b[32m    194\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m vectors = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) != \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[32m    198\u001b[39m     msg = (\n\u001b[32m    199\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mids must be the same length as texts. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ids and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:590\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    589\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_openai/embeddings/base.py:478\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    482\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/openai/resources/embeddings.py:129\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    123\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    124\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m             ).tolist()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/openai/_base_client.py:1042\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/openai/_base_client.py:1124\u001b[39m, in \u001b[36mSyncAPIClient._process_response\u001b[39m\u001b[34m(self, cast_to, options, response, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(response.request.headers.get(RAW_RESPONSE_HEADER)):\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, api_response)\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/openai/_response.py:325\u001b[39m, in \u001b[36mAPIResponse.parse\u001b[39m\u001b[34m(self, to)\u001b[39m\n\u001b[32m    323\u001b[39m parsed = \u001b[38;5;28mself\u001b[39m._parse(to=to)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m._options.post_parser):\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     parsed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, BaseModel):\n\u001b[32m    328\u001b[39m     add_request_id(parsed, \u001b[38;5;28mself\u001b[39m.request_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/openai/resources/embeddings.py:115\u001b[39m, in \u001b[36mEmbeddings.create.<locals>.parser\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_given(encoding_format):\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# don't modify the response object if a user explicitly asked for a format\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "document_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a2e37",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "  (\"human\", \n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise. \"\n",
    "    \"\\n# Question: \\n-> {question} \"\n",
    "    \"\\n# Context: \\n-> {context} \"\n",
    "    \"\\n# Answer: \"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.invoke(\n",
    "  {\"context\": \"##CONTEXT##\", \"question\": \"##QUESTION##\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c56bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "  formatted = \"\\n\\n-> \".join(doc.page_content for doc in docs)\n",
    "  return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1598913",
   "metadata": {},
   "source": [
    "Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When the SIlence of the Lambs was released?\"\n",
    "context = format_docs(retriver.invoke(question))\n",
    "messages = template.invoke({'question': question, 'context': context}).to_messages()\n",
    "answer = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb07e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a73ab5",
   "metadata": {},
   "source": [
    "LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "  RunnableParallel(\n",
    "    context = retriver | format_docs,\n",
    "    question = RunnablePassthrough()\n",
    "  )\n",
    "  | template\n",
    "  | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05152e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"When he was born?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
