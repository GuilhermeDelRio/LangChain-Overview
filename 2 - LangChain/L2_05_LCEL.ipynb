{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48cdb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda, RunnableParallel\n",
    "from langchain_core.tracers.context import collect_runs\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef4c78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  temperature=0.0,\n",
    "  base_url=\"https://openai.vocareum.com/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661c8f3",
   "metadata": {},
   "source": [
    "Chaining invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12a192b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "  template = \"Tell me a joke about {topic}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f6e42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d78c27a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs! üêç‚ú®'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(\n",
    "  llm.invoke(\n",
    "    prompt.invoke(\n",
    "      {\"topic\": \"Python\"}\n",
    "    )\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbee38",
   "metadata": {},
   "source": [
    "Runnables\n",
    "\n",
    "Runnables can be\n",
    "\n",
    "- Executed\n",
    "  - invoke()\n",
    "  - batch()\n",
    "  - and stream()\n",
    "- Inspected\n",
    "- and composed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "892fe689",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnables = [prompt, llm, parser]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c569ca9",
   "metadata": {},
   "source": [
    "Execute methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fea21d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate\n",
      "\\INVOKE: <bound method BasePromptTemplate.invoke of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\tBATCH: <bound method Runnable.batch of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\\STREAM: <bound method Runnable.stream of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "ChatOpenAI\n",
      "\\INVOKE: <bound method BaseChatModel.invoke of ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c3460cc03b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c3461ac73e0>, root_client=<openai.OpenAI object at 0x7c3460cc32f0>, root_async_client=<openai.AsyncOpenAI object at 0x7c3460cc06b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openai.vocareum.com/v1')>\n",
      "\tBATCH: <bound method Runnable.batch of ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c3460cc03b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c3461ac73e0>, root_client=<openai.OpenAI object at 0x7c3460cc32f0>, root_async_client=<openai.AsyncOpenAI object at 0x7c3460cc06b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openai.vocareum.com/v1')>\n",
      "\\STREAM: <bound method BaseChatModel.stream of ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c3460cc03b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c3461ac73e0>, root_client=<openai.OpenAI object at 0x7c3460cc32f0>, root_async_client=<openai.AsyncOpenAI object at 0x7c3460cc06b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openai.vocareum.com/v1')>\n",
      "StrOutputParser\n",
      "\\INVOKE: <bound method BaseOutputParser.invoke of StrOutputParser()>\n",
      "\tBATCH: <bound method Runnable.batch of StrOutputParser()>\n",
      "\\STREAM: <bound method Runnable.stream of StrOutputParser()>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\S'\n",
      "/tmp/ipykernel_181174/2485618608.py:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  print(f\"\\INVOKE: {repr(runnable.invoke)}\")\n",
      "/tmp/ipykernel_181174/2485618608.py:5: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  print(f\"\\STREAM: {repr(runnable.stream)}\")\n"
     ]
    }
   ],
   "source": [
    "for runnable in runnables:\n",
    "  print(f\"{repr(runnable).split('(')[0]}\")\n",
    "  print(f\"\\INVOKE: {repr(runnable.invoke)}\")\n",
    "  print(f\"\\tBATCH: {repr(runnable.batch)}\")\n",
    "  print(f\"\\STREAM: {repr(runnable.stream)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca58112",
   "metadata": {},
   "source": [
    "Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ab99cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate\n",
      "\\INPUT: <class 'langchain_core.utils.pydantic.PromptInput'>\n",
      "\\OUTPUT: <class 'langchain_core.prompts.prompt.PromptTemplateOutput'>\n",
      "\\CONFIG: <class 'langchain_core.utils.pydantic.PromptTemplateConfig'>\n",
      "ChatOpenAI\n",
      "\\INPUT: <class 'langchain_openai.chat_models.base.ChatOpenAIInput'>\n",
      "\\OUTPUT: <class 'langchain_openai.chat_models.base.ChatOpenAIOutput'>\n",
      "\\CONFIG: <class 'langchain_core.utils.pydantic.ChatOpenAIConfig'>\n",
      "StrOutputParser\n",
      "\\INPUT: <class 'langchain_core.output_parsers.string.StrOutputParserInput'>\n",
      "\\OUTPUT: <class 'langchain_core.output_parsers.string.StrOutputParserOutput'>\n",
      "\\CONFIG: <class 'langchain_core.utils.pydantic.StrOutputParserConfig'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\C'\n",
      "/tmp/ipykernel_181174/994800561.py:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  print(f\"\\INPUT: {repr(runnable.get_input_schema())}\")\n",
      "/tmp/ipykernel_181174/994800561.py:4: SyntaxWarning: invalid escape sequence '\\O'\n",
      "  print(f\"\\OUTPUT: {repr(runnable.get_output_schema())}\")\n",
      "/tmp/ipykernel_181174/994800561.py:5: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  print(f\"\\CONFIG: {repr(runnable.config_schema())}\")\n"
     ]
    }
   ],
   "source": [
    "for runnable in runnables:\n",
    "  print(f\"{repr(runnable).split('(')[0]}\")\n",
    "  print(f\"\\INPUT: {repr(runnable.get_input_schema())}\")\n",
    "  print(f\"\\OUTPUT: {repr(runnable.get_output_schema())}\")\n",
    "  print(f\"\\CONFIG: {repr(runnable.config_schema())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c0770",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f375ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with collect_runs() as run_collection:\n",
    "  result = llm.invoke(\n",
    "    \"Hello\",\n",
    "    config={\n",
    "      \"run_name\": \"demo_run\",\n",
    "      \"tags\": [\"demo\", \"lcel\"],\n",
    "      \"metadata\": {'lesson': 2}\n",
    "    }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fac4b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RunTree(id=497271e8-348e-4b26-83eb-863adf8be313, name='demo_run', run_type='llm', dotted_order='20250619T215715051173Z497271e8-348e-4b26-83eb-863adf8be313')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_collection.traced_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3072db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': UUID('497271e8-348e-4b26-83eb-863adf8be313'),\n",
       " 'name': 'demo_run',\n",
       " 'start_time': datetime.datetime(2025, 6, 19, 21, 57, 15, 51173, tzinfo=datetime.timezone.utc),\n",
       " 'run_type': 'llm',\n",
       " 'end_time': datetime.datetime(2025, 6, 19, 21, 57, 16, 181083, tzinfo=datetime.timezone.utc),\n",
       " 'extra': {'invocation_params': {'model': 'gpt-4o-mini',\n",
       "   'model_name': 'gpt-4o-mini',\n",
       "   'stream': False,\n",
       "   'temperature': 0.0,\n",
       "   '_type': 'openai-chat',\n",
       "   'stop': None},\n",
       "  'options': {'stop': None},\n",
       "  'batch_size': 1,\n",
       "  'metadata': {'lesson': 2,\n",
       "   'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.0}},\n",
       " 'error': None,\n",
       " 'serialized': {'lc': 1,\n",
       "  'type': 'constructor',\n",
       "  'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       "  'kwargs': {'model_name': 'gpt-4o-mini',\n",
       "   'temperature': 0.0,\n",
       "   'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']},\n",
       "   'openai_api_base': 'https://openai.vocareum.com/v1'},\n",
       "  'name': 'ChatOpenAI'},\n",
       " 'events': [{'name': 'start',\n",
       "   'time': datetime.datetime(2025, 6, 19, 21, 57, 15, 51173, tzinfo=datetime.timezone.utc)},\n",
       "  {'name': 'end',\n",
       "   'time': datetime.datetime(2025, 6, 19, 21, 57, 16, 181083, tzinfo=datetime.timezone.utc)}],\n",
       " 'inputs': {'prompts': ['Human: Hello']},\n",
       " 'outputs': {'generations': [[{'text': 'Hello! How can I assist you today?',\n",
       "     'generation_info': {'finish_reason': 'stop', 'logprobs': None},\n",
       "     'type': 'ChatGeneration',\n",
       "     'message': {'lc': 1,\n",
       "      'type': 'constructor',\n",
       "      'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
       "      'kwargs': {'content': 'Hello! How can I assist you today?',\n",
       "       'additional_kwargs': {'refusal': None},\n",
       "       'response_metadata': {'token_usage': {'completion_tokens': 9,\n",
       "         'prompt_tokens': 8,\n",
       "         'total_tokens': 17,\n",
       "         'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "          'audio_tokens': 0,\n",
       "          'reasoning_tokens': 0,\n",
       "          'rejected_prediction_tokens': 0},\n",
       "         'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "        'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "        'system_fingerprint': 'fp_34a54ae93c',\n",
       "        'id': 'chatcmpl-BkHXv0ELis36eXSPsQhDkay5ALEWp',\n",
       "        'service_tier': 'default',\n",
       "        'finish_reason': 'stop',\n",
       "        'logprobs': None},\n",
       "       'type': 'ai',\n",
       "       'id': 'run--497271e8-348e-4b26-83eb-863adf8be313-0',\n",
       "       'usage_metadata': {'input_tokens': 8,\n",
       "        'output_tokens': 9,\n",
       "        'total_tokens': 17,\n",
       "        'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "        'output_token_details': {'audio': 0, 'reasoning': 0}},\n",
       "       'tool_calls': [],\n",
       "       'invalid_tool_calls': []}}}]],\n",
       "  'llm_output': {'token_usage': {'completion_tokens': 9,\n",
       "    'prompt_tokens': 8,\n",
       "    'total_tokens': 17,\n",
       "    'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "     'audio_tokens': 0,\n",
       "     'reasoning_tokens': 0,\n",
       "     'rejected_prediction_tokens': 0},\n",
       "    'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "   'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "   'system_fingerprint': 'fp_34a54ae93c',\n",
       "   'id': 'chatcmpl-BkHXv0ELis36eXSPsQhDkay5ALEWp',\n",
       "   'service_tier': 'default'},\n",
       "  'run': None,\n",
       "  'type': 'LLMResult'},\n",
       " 'reference_example_id': None,\n",
       " 'parent_run_id': None,\n",
       " 'tags': ['demo', 'lcel'],\n",
       " 'attachments': {},\n",
       " 'child_runs': [],\n",
       " 'session_name': 'default',\n",
       " 'session_id': None,\n",
       " 'dotted_order': '20250619T215715051173Z497271e8-348e-4b26-83eb-863adf8be313',\n",
       " 'trace_id': UUID('497271e8-348e-4b26-83eb-863adf8be313'),\n",
       " 'dangerously_allow_filesystem': False,\n",
       " 'replicas': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_collection.traced_runs[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d7687b",
   "metadata": {},
   "source": [
    "Compose Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31185f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableSequence(prompt, llm, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "680428c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21ad615a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs! üêç‚ú®'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"Python\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91ce9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do Python programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"topic\": \"Python\"}):\n",
    "  print(chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36926f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!',\n",
       " 'Why did the JavaScript developer go broke?\\n\\nBecause he kept using \"null\" as a value!',\n",
       " 'Why do programmers prefer dark mode in Java?\\n\\nBecause light attracts bugs!']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\n",
    "  {\"topic\": \"Python\"},\n",
    "  {\"topic\": \"JavaScript\"},\n",
    "  {\"topic\": \"Java\"}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d38288b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install grandalf to draw graphs: `pip install grandalf`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_core/runnables/graph_ascii.py:178\u001b[39m, in \u001b[36m_build_sugiyama_layout\u001b[39m\u001b[34m(vertices, edges)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrandalf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Edge, Graph, Vertex  \u001b[38;5;66;03m# type: ignore[import-untyped]\u001b[39;00m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrandalf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayouts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SugiyamaLayout  \u001b[38;5;66;03m# type: ignore[import-untyped]\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'grandalf'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py:526\u001b[39m, in \u001b[36mGraph.print_ascii\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_ascii\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    525\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Print the graph as an ASCII art string.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdraw_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_core/runnables/graph.py:519\u001b[39m, in \u001b[36mGraph.draw_ascii\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    516\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Draw the graph as an ASCII art string.\"\"\"\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_ascii\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_ascii\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_ascii\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_core/runnables/graph_ascii.py:272\u001b[39m, in \u001b[36mdraw_ascii\u001b[39m\u001b[34m(vertices, edges)\u001b[39m\n\u001b[32m    269\u001b[39m xlist: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m] = []\n\u001b[32m    270\u001b[39m ylist: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m sug = \u001b[43m_build_sugiyama_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vertex \u001b[38;5;129;01min\u001b[39;00m sug.g.sV:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# NOTE: moving boxes w/2 to the left\u001b[39;00m\n\u001b[32m    276\u001b[39m     xlist.extend(\n\u001b[32m    277\u001b[39m         (\n\u001b[32m    278\u001b[39m             vertex.view.xy[\u001b[32m0\u001b[39m] - vertex.view.w / \u001b[32m2.0\u001b[39m,\n\u001b[32m    279\u001b[39m             vertex.view.xy[\u001b[32m0\u001b[39m] + vertex.view.w / \u001b[32m2.0\u001b[39m,\n\u001b[32m    280\u001b[39m         )\n\u001b[32m    281\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Cursos/Udacity/langChain/venv/lib/python3.12/site-packages/langchain_core/runnables/graph_ascii.py:183\u001b[39m, in \u001b[36m_build_sugiyama_layout\u001b[39m\u001b[34m(vertices, edges)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    182\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInstall grandalf to draw graphs: `pip install grandalf`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Just a reminder about naming conventions:\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# +------------X\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Y\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    195\u001b[39m vertices_ = {id_: Vertex(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m id_, data \u001b[38;5;129;01min\u001b[39;00m vertices.items()}\n",
      "\u001b[31mImportError\u001b[39m: Install grandalf to draw graphs: `pip install grandalf`."
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5655d2",
   "metadata": {},
   "source": [
    "Turn any function into a runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6356a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x: int) -> int:\n",
    "  return x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fa742cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableLambda(double)\n",
    "runnable.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c0f20",
   "metadata": {},
   "source": [
    "Parallel Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1fcd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "parrallel_chain = RunnableParallel(\n",
    "  double=RunnableLambda(lambda x: x * 2),\n",
    "  triple=RunnableLambda(lambda x: x * 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8087bc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'double': 4, 'triple': 6}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parrallel_chain.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc7465",
   "metadata": {},
   "source": [
    "LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebf20e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec7a47bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c3460cc03b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c3461ac73e0>, root_client=<openai.OpenAI object at 0x7c3460cc32f0>, root_async_client=<openai.AsyncOpenAI object at 0x7c3460cc06b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openai.vocareum.com/v1')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bc3d804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StrOutputParser()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be51b6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c3460cc03b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c3461ac73e0>, root_client=<openai.OpenAI object at 0x7c3460cc32f0>, root_async_client=<openai.AsyncOpenAI object at 0x7c3460cc06b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openai.vocareum.com/v1')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain  = RunnableSequence(prompt, llm, parser)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "857371a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c3460cc03b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c3461ac73e0>, root_client=<openai.OpenAI object at 0x7c3460cc32f0>, root_async_client=<openai.AsyncOpenAI object at 0x7c3460cc06b0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openai.vocareum.com/v1')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c366d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bab948b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the computer go to therapy?\\n\\nBecause it had too many bytes from its past!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"Computer\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
